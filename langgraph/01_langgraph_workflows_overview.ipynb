{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "572a4837",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: flex-start; align-items: center; margin-bottom: 20px;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/aparsoft/aparsoft-tutorial-resources/main/assets/icons/aparsoft_logo.png\" alt=\"Aparsoft Logo\" style=\"height: 60px;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9fd26a",
   "metadata": {},
   "source": [
    "# Langgraph: Workflows Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c4545",
   "metadata": {},
   "source": [
    "# Quickly understanding Workflows and agents\n",
    "\n",
    "Reviews common workflow and agent patterns.\n",
    "\n",
    "* Workflows have predetermined code paths and are designed to operate in a certain order.\n",
    "* Agents are dynamic and define their own processes and tool usage.\n",
    "\n",
    "![png](01_langgraph_workflows_and_agents_files/agent_workflow.png)\n",
    "\n",
    "LangGraph offers several benefits when building agents and workflows, including [persistence](/oss/python/langgraph/persistence), [streaming](/oss/python/langgraph/streaming), and support for debugging as well as [deployment](/oss/python/langgraph/deploy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63cde7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a secret openai key input in jupyter notebook\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Securely input OpenAI API key\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "515d158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define openai llm\n",
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"o4-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1a8f4e",
   "metadata": {},
   "source": [
    "## LLMs and augmentations\n",
    "\n",
    "Workflows and agentic systems are based on LLMs and the various augmentations you add to them. [Tool calling](/oss/python/langchain/tools), [structured outputs](/oss/python/langchain/structured-output), and [short term memory](/oss/python/langchain/short-term-memory) are a few options for tailoring LLMs to your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72afe3",
   "metadata": {},
   "source": [
    "![png](01_langgraph_workflows_and_agents_files/augmented_llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bf61fa",
   "metadata": {},
   "source": [
    "### Define a structured output schema using pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9fd8f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'call_f5sotq6ebfiqxoLU4S9m8kAM',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a structured output schema using pydantic\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n",
    "    justification: str = Field(\n",
    "        None, description=\"Why this query is relevant to the user's request.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "structured_llm = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "# Invoke the augmented LLM\n",
    "output = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n",
    "\n",
    "\n",
    "# Define a tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# Augment the LLM with tools\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "\n",
    "# Invoke the LLM with input that triggers the tool call\n",
    "msg = llm_with_tools.invoke(\"What is 2 times 3?\")\n",
    "\n",
    "# Get the tool call\n",
    "msg.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a68a2a",
   "metadata": {},
   "source": [
    "## Prompt chaining\n",
    "\n",
    "Prompt chaining is when each LLM call processes the output of the previous call. It's often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include:\n",
    "\n",
    "* Translating documents into different languages\n",
    "* Verifying generated content for consistency\n",
    "\n",
    "![png](01_langgraph_workflows_and_agents_files/prompt_chain.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "259dd185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "# Define graph state schema\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    improved_joke: str\n",
    "    final_joke: str\n",
    "\n",
    "\n",
    "# Define graph nodes\n",
    "def generate_joke(state: State):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "# Define a gate function\n",
    "def check_punchline(state: State):\n",
    "    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n",
    "\n",
    "    # Simple check - does the joke contain \"?\" or \"!\"\n",
    "    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n",
    "        return \"Pass\"\n",
    "    return \"Fail\"\n",
    "\n",
    "# Second LLM call\n",
    "def improve_joke(state: State):\n",
    "    \"\"\"Second LLM call to improve the joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n",
    "    return {\"improved_joke\": msg.content}\n",
    "\n",
    "# Third LLM call\n",
    "def polish_joke(state: State):\n",
    "    \"\"\"Third LLM call for final polish\"\"\"\n",
    "    msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n",
    "    return {\"final_joke\": msg.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b97d86",
   "metadata": {},
   "source": [
    "### Build the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a323b004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"generate_joke\", generate_joke)\n",
    "workflow.add_node(\"improve_joke\", improve_joke)\n",
    "workflow.add_node(\"polish_joke\", polish_joke)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "workflow.add_edge(START, \"generate_joke\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\n",
    ")\n",
    "workflow.add_edge(\"improve_joke\", \"polish_joke\")\n",
    "workflow.add_edge(\"polish_joke\", END)\n",
    "\n",
    "# Compile the graph into a workflow\n",
    "chain = workflow.compile()\n",
    "\n",
    "# Display the graph\n",
    "display(Image(chain.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b49b529",
   "metadata": {},
   "source": [
    "### Invoke LLM graph (workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ea630f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial joke:\n",
      "What do clouds wear under their raincoats?  \n",
      "Thunderwear!\n",
      "\n",
      "--- --- ---\n",
      "\n",
      "Joke failed quality gate - no punchline detected!\n"
     ]
    }
   ],
   "source": [
    "# Invoke\n",
    "state = chain.invoke({\"topic\": \"clouds\"})\n",
    "print(\"Initial joke:\")\n",
    "print(state[\"joke\"])\n",
    "print(\"\\n--- --- ---\\n\")\n",
    "if \"improved_joke\" in state:\n",
    "    print(\"Improved joke:\")\n",
    "    print(state[\"improved_joke\"])\n",
    "    print(\"\\n--- --- ---\\n\")\n",
    "\n",
    "    print(\"Final joke:\")\n",
    "    print(state[\"final_joke\"])\n",
    "else:\n",
    "    print(\"Joke failed quality gate - no punchline detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f4b943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'generate_joke': 'What did one cloud say to the other?  \\n“I’m feeling under the weather.”'}\n",
      "\n",
      "\n",
      "{'improve_joke': 'Here’s a punchier, pun-packed version:\\n\\nWhat did one cloud say to the other?  \\n“I’m feeling under the weather.”  \\nThe other sighed, “Sounds like you’ve got a serious case of cumulo-stress—time for some cirrus relief!”'}\n",
      "\n",
      "\n",
      "{'polish_joke': 'Here’s the original with a little snow-globe shake-up twist:\\n\\nWhat did one cloud say to the other?  \\n“I’m feeling under the weather.”  \\nThe other sighed, “Sounds like you’ve got a serious case of cumulo-stress—time for some cirrus relief!”  \\n\\nJust then the whole sky rattled. A tiny voice laughed, “Oops—did I shake the snow globe again?”  \\nTurns out those clouds weren’t on a forecast at all, but starring in Billy’s desk accessory.'}\n",
      "\n",
      "\n",
      "{'prompt_chaining_workflow': 'Here’s the original with a little snow-globe shake-up twist:\\n\\nWhat did one cloud say to the other?  \\n“I’m feeling under the weather.”  \\nThe other sighed, “Sounds like you’ve got a serious case of cumulo-stress—time for some cirrus relief!”  \\n\\nJust then the whole sky rattled. A tiny voice laughed, “Oops—did I shake the snow globe again?”  \\nTurns out those clouds weren’t on a forecast at all, but starring in Billy’s desk accessory.'}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langgraph.func import entrypoint, task\n",
    "\n",
    "\n",
    "# Tasks\n",
    "@task\n",
    "def generate_joke(topic: str):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "    msg = llm.invoke(f\"Write a short joke about {topic}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def check_punchline(joke: str):\n",
    "    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n",
    "    # Simple check - does the joke contain \"?\" or \"!\"\n",
    "    if \"?\" in joke or \"!\" in joke:\n",
    "        return \"Fail\"\n",
    "\n",
    "    return \"Pass\"\n",
    "\n",
    "\n",
    "@task\n",
    "def improve_joke(joke: str):\n",
    "    \"\"\"Second LLM call to improve the joke\"\"\"\n",
    "    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@task\n",
    "def polish_joke(joke: str):\n",
    "    \"\"\"Third LLM call for final polish\"\"\"\n",
    "    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "def prompt_chaining_workflow(topic: str):\n",
    "    original_joke = generate_joke(topic).result()\n",
    "    if check_punchline(original_joke) == \"Pass\":\n",
    "        return original_joke\n",
    "\n",
    "    improved_joke = improve_joke(original_joke).result()\n",
    "    return polish_joke(improved_joke).result()\n",
    "\n",
    "\n",
    "# Invoke\n",
    "for step in prompt_chaining_workflow.stream(\"clouds\", stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a7e0b",
   "metadata": {},
   "source": [
    "## Parallelization\n",
    "\n",
    "With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to:\n",
    "\n",
    "* Split up subtasks and run them in parallel, which increases speed\n",
    "* Run tasks multiple times to check for different outputs, which increases confidence\n",
    "\n",
    "Some examples include:\n",
    "\n",
    "* Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors\n",
    "* Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources\n",
    "\n",
    "![png](01_langgraph_workflows_and_agents_files/parallelization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a14d2f",
   "metadata": {},
   "source": [
    "### Parallelization example\n",
    "\n",
    "The task is to generate a joke about a given topic, improve it, and then create a polished a story and poem on the same topic.\n",
    "\n",
    "The workflow has three steps:\n",
    "1. Generate a joke about a given topic\n",
    "2. Generate story on the same topic\n",
    "3. Generate a poem on the same topic\n",
    "The task of generating the joke, story and poem are independent of each other, so they can be run in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959ddba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define graph state\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    joke: str\n",
    "    story: str\n",
    "    poem: str\n",
    "    combined_output: str\n",
    "\n",
    "\n",
    "# Define graph nodes. These tasks can be run in parallel\n",
    "# First LLM call: Generate joke\n",
    "def call_llm_1(state: State):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "# Second LLM call: Generate story\n",
    "def call_llm_2(state: State):\n",
    "    \"\"\"Second LLM call to generate story\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n",
    "    return {\"story\": msg.content}\n",
    "\n",
    "# Third LLM call: Generate poem\n",
    "def call_llm_3(state: State):\n",
    "    \"\"\"Third LLM call to generate poem\"\"\"\n",
    "\n",
    "    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n",
    "    return {\"poem\": msg.content}\n",
    "\n",
    "# Aggregator node\n",
    "def aggregator(state: State):\n",
    "    \"\"\"Combine the joke and story into a single output\"\"\"\n",
    "\n",
    "    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n",
    "    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n",
    "    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n",
    "    combined += f\"POEM:\\n{state['poem']}\"\n",
    "    return {\"combined_output\": combined}\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "parallel_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "parallel_builder.add_node(\"call_llm_1\", call_llm_1)\n",
    "parallel_builder.add_node(\"call_llm_2\", call_llm_2)\n",
    "parallel_builder.add_node(\"call_llm_3\", call_llm_3)\n",
    "parallel_builder.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "parallel_builder.add_edge(START, \"call_llm_1\")\n",
    "parallel_builder.add_edge(START, \"call_llm_2\")\n",
    "parallel_builder.add_edge(START, \"call_llm_3\")\n",
    "parallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\n",
    "parallel_builder.add_edge(\"aggregator\", END)\n",
    "parallel_workflow = parallel_builder.compile()\n",
    "\n",
    "# Show workflow\n",
    "display(Image(parallel_workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7aa1dc",
   "metadata": {},
   "source": [
    "### Invoke the parallel LLM graph (workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41bf246a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a story, joke, and poem about cats!\n",
      "\n",
      "STORY:\n",
      "Long ago, in a sleepy village tucked between rolling green hills, three cats made their homes among the ivy-covered rooftops. There was Luna, with silvery fur that gleamed like moonlight; Ember, a tiny ginger kitten whose boundless curiosity led her into every nook and cranny; and Shadow, a sleek black tom whose calm, watchful eyes missed nothing. Each evening, the three friends gathered on a low rooftop to swap stories of their day: Luna’s graceful prowls, Ember’s new discoveries, Shadow’s patient observations.\n",
      "\n",
      "One night, as the moon rose full and golden, Ember bounded back to the meeting spot, her whiskers quivering with excitement. “Come look!” she cried. Beneath a broken tile in the roof’s edge, she’d uncovered a small wooden door, carved with paw-prints and ivy leaves. Luna nudged it open, revealing a narrow staircase winding down into darkness. With Shadow padding quietly at her side, Luna led the way. Their heartbeats quickened as they descended, lantern-light dancing on damp stone.\n",
      "\n",
      "At the bottom, they entered a hidden garden, bathed in moonlight that seeped through cracks in the ceiling. Flowers glowed in soft blues and purples, and at its center bubbled a crystal-clear fountain. As the cats approached, a gentle voice spoke from the shadows—a silver fox, ancient and wise, who introduced herself as Aria, guardian of this secret place. She told them the fountain’s water could heal wounded hearts and weary paws. Ember dipped a paw in the fountain and tasted the cool water; at once she felt braver and more joyful than ever. Luna and Shadow followed suit, each discovering newfound courage and peace.\n",
      "\n",
      "When the first rays of dawn filtered through the stones, Aria bade them to keep the garden’s magic safe. The trio climbed back to the rooftops, spirits lifted, their bond stronger than before. Now, beneath every silvery moon, Luna, Ember, and Shadow meet in whispered purrs to remember the hidden garden—and to promise that as long as they watch over their village, wonder will never fade from their world.\n",
      "\n",
      "JOKE:\n",
      "Why don’t cats play poker in the jungle?  \n",
      "Because there are way too many cheetahs!\n",
      "\n",
      "POEM:\n",
      "Soft pawprints trace the morning light,  \n",
      "Whiskers twitching at dawn’s first glow.  \n",
      "Velvet bodies curled just right,  \n",
      "Dreaming of the mice they’ll never know.  \n",
      "\n",
      "Emerald eyes in shadowed grace,  \n",
      "Gaze upon the world with secret pride.  \n",
      "Silent hunters, they roam each place,  \n",
      "Slinking where our hearts confide.  \n",
      "\n",
      "At noon they stretch on sunlit sills,  \n",
      "A lazy symphony of quiet yawns.  \n",
      "Their purrs weave warmth, each heartbeat fills  \n",
      "The empty spaces in our lawns.  \n",
      "\n",
      "Night unfurls its starry sky,  \n",
      "Cat’s soft meow drifts on the breeze.  \n",
      "They vanish while we wonder why,  \n",
      "Then reappear with graceful ease.  \n",
      "\n",
      "In every leap, in every stare,  \n",
      "In gentle brush of fur so fine,  \n",
      "Cats bring the magic everywhere—  \n",
      "A whispered touch of the divine.\n"
     ]
    }
   ],
   "source": [
    "# Invoke\n",
    "state = parallel_workflow.invoke({\"topic\": \"cats\"})\n",
    "print(state[\"combined_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4cd083",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def call_llm_1(topic: str):\n",
    "    \"\"\"First LLM call to generate initial joke\"\"\"\n",
    "    msg = llm.invoke(f\"Write a joke about {topic}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@task\n",
    "def call_llm_2(topic: str):\n",
    "    \"\"\"Second LLM call to generate story\"\"\"\n",
    "    msg = llm.invoke(f\"Write a story about {topic}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@task\n",
    "def call_llm_3(topic):\n",
    "    \"\"\"Third LLM call to generate poem\"\"\"\n",
    "    msg = llm.invoke(f\"Write a poem about {topic}\")\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "@task\n",
    "def aggregator(topic, joke, story, poem):\n",
    "    \"\"\"Combine the joke and story into a single output\"\"\"\n",
    "\n",
    "    combined = f\"Here's a story, joke, and poem about {topic}!\\n\\n\"\n",
    "    combined += f\"STORY:\\n{story}\\n\\n\"\n",
    "    combined += f\"JOKE:\\n{joke}\\n\\n\"\n",
    "    combined += f\"POEM:\\n{poem}\"\n",
    "    return combined\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "@entrypoint()\n",
    "def parallel_workflow(topic: str):\n",
    "    joke_fut = call_llm_1(topic)\n",
    "    story_fut = call_llm_2(topic)\n",
    "    poem_fut = call_llm_3(topic)\n",
    "    return aggregator(\n",
    "        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\n",
    "    ).result()\n",
    "\n",
    "\n",
    "# Invoke\n",
    "for step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2639a45",
   "metadata": {},
   "source": [
    "## Routing\n",
    "\n",
    "Routing workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.\n",
    "\n",
    "![routing.png](01_langgraph_workflows_and_agents_files/routing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cf7eff",
   "metadata": {},
   "source": [
    "### Example for routing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12e01eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "# Schema for structured output to use as routing logic\n",
    "class Route(BaseModel):\n",
    "    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n",
    "        None, description=\"The next step in the routing process\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "router = llm.with_structured_output(Route)\n",
    "\n",
    "\n",
    "# State\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    decision: str\n",
    "    output: str\n",
    "\n",
    "\n",
    "# Define graph nodes\n",
    "# LLM call to write a story\n",
    "def llm_call_1(state: State):\n",
    "    \"\"\"Write a story\"\"\"\n",
    "\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "# LLM call to write a joke\n",
    "def llm_call_2(state: State):\n",
    "    \"\"\"Write a joke\"\"\"\n",
    "\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "# LLM call to write a poem\n",
    "def llm_call_3(state: State):\n",
    "    \"\"\"Write a poem\"\"\"\n",
    "\n",
    "    result = llm.invoke(state[\"input\"])\n",
    "    return {\"output\": result.content}\n",
    "\n",
    "# LLM call to route the input\n",
    "def llm_call_router(state: State):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "\n",
    "    # Run the augmented LLM with structured output to serve as routing logic\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Route the input to story, joke, or poem based on the user's request.\"\n",
    "            ),\n",
    "            HumanMessage(content=state[\"input\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(f\"Routing decision: {decision.step}\")\n",
    "\n",
    "    return {\"decision\": decision.step}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b276e75",
   "metadata": {},
   "source": [
    "### Conditionally route the input to different nodes based on the content of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a34126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional edge function to route to the appropriate node\n",
    "def route_decision(state: State):\n",
    "    # Return the node name you want to visit next\n",
    "    if state[\"decision\"] == \"story\":\n",
    "        return \"llm_call_1\"\n",
    "    elif state[\"decision\"] == \"joke\":\n",
    "        return \"llm_call_2\"\n",
    "    elif state[\"decision\"] == \"poem\":\n",
    "        return \"llm_call_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5070f36",
   "metadata": {},
   "source": [
    "### Build the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f6ca831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build workflow\n",
    "router_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "router_builder.add_node(\"llm_call_1\", llm_call_1)\n",
    "router_builder.add_node(\"llm_call_2\", llm_call_2)\n",
    "router_builder.add_node(\"llm_call_3\", llm_call_3)\n",
    "router_builder.add_node(\"llm_call_router\", llm_call_router)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "router_builder.add_edge(START, \"llm_call_router\")\n",
    "router_builder.add_conditional_edges(\n",
    "    \"llm_call_router\",\n",
    "    route_decision,\n",
    "    {  # Name returned by route_decision : Name of next node to visit\n",
    "        \"llm_call_1\": \"llm_call_1\",\n",
    "        \"llm_call_2\": \"llm_call_2\",\n",
    "        \"llm_call_3\": \"llm_call_3\",\n",
    "    },\n",
    ")\n",
    "router_builder.add_edge(\"llm_call_1\", END)\n",
    "router_builder.add_edge(\"llm_call_2\", END)\n",
    "router_builder.add_edge(\"llm_call_3\", END)\n",
    "\n",
    "# Compile workflow\n",
    "router_workflow = router_builder.compile()\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(router_workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08ec666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Routing decision: joke\n",
      "Why don’t cats play poker in the jungle?  \n",
      "Too many cheetahs!\n"
     ]
    }
   ],
   "source": [
    "# Invoke\n",
    "state = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\n",
    "print(state[\"output\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f190ce5",
   "metadata": {},
   "source": [
    "### Functional API code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "# Schema for structured output to use as routing logic\n",
    "class Route(BaseModel):\n",
    "    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n",
    "        None, description=\"The next step in the routing process\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "router = llm.with_structured_output(Route)\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call_1(input_: str):\n",
    "    \"\"\"Write a story\"\"\"\n",
    "    result = llm.invoke(input_)\n",
    "    return result.content\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call_2(input_: str):\n",
    "    \"\"\"Write a joke\"\"\"\n",
    "    result = llm.invoke(input_)\n",
    "    return result.content\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call_3(input_: str):\n",
    "    \"\"\"Write a poem\"\"\"\n",
    "    result = llm.invoke(input_)\n",
    "    return result.content\n",
    "\n",
    "\n",
    "def llm_call_router(input_: str):\n",
    "    \"\"\"Route the input to the appropriate node\"\"\"\n",
    "    # Run the augmented LLM with structured output to serve as routing logic\n",
    "    decision = router.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Route the input to story, joke, or poem based on the user's request.\"\n",
    "            ),\n",
    "            HumanMessage(content=input_),\n",
    "        ]\n",
    "    )\n",
    "    return decision.step\n",
    "\n",
    "\n",
    "# Create workflow\n",
    "@entrypoint()\n",
    "def router_workflow(input_: str):\n",
    "    next_step = llm_call_router(input_)\n",
    "    if next_step == \"story\":\n",
    "        llm_call = llm_call_1\n",
    "    elif next_step == \"joke\":\n",
    "        llm_call = llm_call_2\n",
    "    elif next_step == \"poem\":\n",
    "        llm_call = llm_call_3\n",
    "\n",
    "    return llm_call(input_).result()\n",
    "\n",
    "\n",
    "# Invoke\n",
    "for step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\n",
    "    print(step)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3927e",
   "metadata": {},
   "source": [
    "## Orchestrator-worker\n",
    "\n",
    "In an orchestrator-worker configuration, the orchestrator:\n",
    "\n",
    "* Breaks down tasks into subtasks\n",
    "* Delegates subtasks to workers\n",
    "* Synthesizes worker outputs into a final result\n",
    "\n",
    "![png](01_langgraph_workflows_and_agents_files/worker.png)\n",
    "\n",
    "Orchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with [parallelization](#parallelization). This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7af45",
   "metadata": {},
   "source": [
    "### Orchestrator-worker example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa03eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "import operator\n",
    "\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "# Define a section of the report\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "# Define the full report structure\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79bbb7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7cec2d72eff0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7cec2d387710>, root_client=<openai.OpenAI object at 0x7cec2e339070>, root_async_client=<openai.AsyncOpenAI object at 0x7cec2d4611c0>, model_name='o4-mini', model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'response_format': <class '__main__.Sections'>, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'Sections', 'description': '', 'parameters': {'properties': {'sections': {'description': 'Sections of the report.', 'items': {'properties': {'name': {'description': 'Name for this section of the report.', 'type': 'string'}, 'description': {'description': 'Brief overview of the main topics and concepts to be covered in this section.', 'type': 'string'}}, 'required': ['name', 'description'], 'type': 'object'}, 'type': 'array'}}, 'required': ['sections'], 'type': 'object'}}}}}, config={}, config_factories=[])\n",
       "| RunnableBinding(bound=RunnableLambda(...), kwargs={}, config={}, config_factories=[], custom_output_type=<class '__main__.Sections'>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "planner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e729412",
   "metadata": {},
   "source": [
    "### Creating workers in LangGraph\n",
    "\n",
    "Orchestrator-worker workflows are common and LangGraph has built-in support for them. The `Send` API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the `Send` API to send a section to each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8eddc06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.types import Send\n",
    "\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str  # Report topic\n",
    "    sections: list[Section]  # List of report sections\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel\n",
    "    final_report: str  # Final report\n",
    "\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]\n",
    "\n",
    "\n",
    "# Define graph nodes\n",
    "# Orchestrator node to get the sections of the report\n",
    "def orchestrator(state: State):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Generated sections: {report_sections.sections}\")\n",
    "\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "# Worker node for writing a section of the report\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    print(f\"Completed section: {section.content}\")\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section.content]}\n",
    "\n",
    "# Aggregator node\n",
    "def synthesizer(state: State):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections}\n",
    "\n",
    "\n",
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a3732c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sections: [Section(name='Executive Summary', description='A concise overview of the report’s objectives, key findings on LLM scaling laws, and their implications for research and industry.'), Section(name='Introduction', description='Define large language models (LLMs), outline the importance of understanding scaling behavior, and state the report’s scope and goals.'), Section(name='Background on Large Language Models', description='Review the architecture, training paradigms, and historical evolution of LLMs to establish context for scaling discussions.'), Section(name='Empirical Observations of Scaling Laws', description='Summarize major experimental studies showing how model performance scales with compute, data, and parameters; present key curves and benchmark results.'), Section(name='Theoretical Foundations of Scaling Laws', description='Examine theoretical explanations—such as power-law distributions, neural tangent kernels, and double descent phenomena—that underlie observed scaling behaviors.'), Section(name='Methodologies for Analyzing Scaling', description='Detail the experimental setups, metrics, data regimes, and statistical techniques used to measure and validate scaling trends in LLMs.'), Section(name='Practical Implications for Model Development', description='Discuss how scaling laws inform decisions on model size, data collection, compute budgeting, and deployment strategies for real-world applications.'), Section(name='Limitations and Challenges', description='Identify gaps in current scaling law research, including biases, environmental costs, reproducibility issues, and diminishing returns at extreme scales.'), Section(name='Future Directions', description='Propose avenues for advancing scaling law studies, such as multimodal extensions, new theoretical models, efficient scaling methods, and sustainability-oriented research.'), Section(name='Conclusion', description='Recap the main insights on LLM scaling laws, their significance for future AI progress, and the strategic considerations for stakeholders.')]\n",
      "Completed section: # Introduction\n",
      "\n",
      "Large language models (LLMs) are deep neural networks—typically built on transformer architectures—that are pretrained on vast corpora of text data to perform a wide range of natural language processing tasks. By learning statistical patterns and contextual relationships across billions or even trillions of tokens, LLMs can generate coherent text, answer questions, translate languages, summarize documents, and adapt to many downstream applications with minimal fine-tuning.\n",
      "\n",
      "Understanding how these models behave as they grow in size (“scaling behavior”) is critical for both researchers and practitioners. Empirical scaling laws reveal predictable improvements in performance with increasing model parameters, training data, and compute resources. At the same time, scaling can introduce emergent capabilities, shifting cost-benefit trade-offs, and new risks such as amplification of biases or vulnerabilities. A clear grasp of scaling behavior informs decisions about resource allocation, model deployment, and alignment strategies, ensuring that model development remains both efficient and responsible.\n",
      "\n",
      "This report investigates the scaling behavior of LLMs with three main goals:\n",
      "1. Characterize how performance metrics (e.g., perplexity, downstream task accuracy) systematically improve with model size, data volume, and compute budgets.  \n",
      "2. Identify key points of diminishing returns and explore strategies—such as model sparsity or data curation—to mitigate escalating costs.  \n",
      "3. Assess broader implications of scaling, including emergent behaviors, ethical considerations, and sustainability, to inform best practices for future model development.\n",
      "\n",
      "By mapping out the relationship between scale and capability, this report aims to guide stakeholders in making informed, cost-effective, and responsible choices in the era of ever-larger language models.\n",
      "Completed section: # Executive Summary\n",
      "\n",
      "This report investigates the scaling laws that govern the performance of large language models (LLMs) as a function of compute, model size, and data. Our objectives are to:\n",
      "\n",
      "- Characterize empirical relationships between model parameters, training tokens, and downstream task performance.  \n",
      "- Identify regimes of compute- and data-efficiency.  \n",
      "- Provide actionable guidance for researchers and industry practitioners on resource allocation and model design.\n",
      "\n",
      "Key Findings on LLM Scaling Laws  \n",
      "- **Power-Law Behavior**: Model performance (e.g., loss, perplexity) improves predictably with model size and training compute, following a power-law trend across multiple architectures.  \n",
      "- **Diminishing Returns**: Beyond certain compute thresholds, marginal gains per extra parameter or token decline, highlighting a regime shift from compute-bound to data-bound performance.  \n",
      "- **Optimal Compute Allocation**: There exists a “compute-optimal frontier” where the ratio of model size to training tokens maximizes performance per dollar spent.  \n",
      "- **Transfer and Fine-Tuning**: Larger pre-trained models yield steeper transfer curves, reducing fine-tuning data requirements for downstream tasks.\n",
      "\n",
      "Implications for Research and Industry  \n",
      "- **Resource Planning**: Organizations can forecast performance improvements and budget more accurately by adopting power-law projections.  \n",
      "- **Model Design**: The compute-optimal frontier helps determine when to scale model size versus invest in more training data or compute infrastructure.  \n",
      "- **Data Strategy**: In data-bound regimes, efforts should prioritize high-quality or diverse corpora rather than raw token count.  \n",
      "- **Competitive Advantage**: Early adopters of appropriately scaled models can achieve significant performance gains on specialized tasks with lower incremental costs.\n",
      "\n",
      "By grounding decisions in empirically validated scaling laws, stakeholders can optimize R&D investments, accelerate innovation, and balance performance objectives against resource constraints.\n",
      "Completed section: ## Conclusion\n",
      "\n",
      "Over the past decade, empirical scaling laws have revealed remarkably consistent power-law relationships between model performance and three primary resources—parameter count, training data volume, and compute. Key insights include:  \n",
      "- Predictable gains: Each doubling of compute or data tends to yield a reproducible improvement in cross-task performance metrics.  \n",
      "- Diminishing returns boundaries: While raw scale drives breakthroughs, marginal gains shrink at extreme sizes, highlighting the importance of algorithmic and architectural innovations.  \n",
      "- Emergent capabilities: At critical scales, models exhibit qualitatively new abilities—reasoning, code synthesis, multi-modal understanding—that were not present at smaller sizes.\n",
      "\n",
      "These findings carry profound implications for the future trajectory of AI:  \n",
      "- Roadmap for innovation: Scaling laws afford a quantitative blueprint for resource allocation, enabling research teams to plan compute and data investments more strategically.  \n",
      "- Acceleration of transformative applications: Continued scale-driven improvements promise advances in healthcare, climate modeling, scientific discovery, and creative industries.  \n",
      "- New frontiers in safety and alignment: As models become more capable, ensuring robust grounding, interpretability, and alignment with human values grows ever more critical.\n",
      "\n",
      "Stakeholders across academia, industry, and government must consider:  \n",
      "- Researchers and developers: Balance brute-force scaling with efficiency gains via sparse architectures, data curation, and novel training paradigms.  \n",
      "- Corporations and startups: Invest in modular AI stacks and cloud-based infrastructure to remain agile, while forging partnerships to access specialized datasets and hardware.  \n",
      "- Policymakers and regulators: Craft adaptive frameworks that encourage innovation yet mitigate risks—covering transparency standards, accountability measures, and mechanisms for international coordination.  \n",
      "- Society and educators: Prepare the workforce through reskilling initiatives, emphasize AI literacy, and foster public dialogues on ethical deployment.\n",
      "\n",
      "By heeding these insights and aligning strategic decisions with scaling-law principles, stakeholders can steer AI development toward more powerful, beneficial, and responsible outcomes.\n",
      "Completed section: ## Future Directions\n",
      "\n",
      "### 1. Multimodal Scaling Laws  \n",
      "- **Cross-Modal Interactions**: Empirically characterize how scaling behaviors transfer between modalities (e.g., vision ↔ language) and identify unified power-law exponents.  \n",
      "- **Composite Training Curves**: Develop joint scaling functions that predict performance when training on combined text, image, audio, and video datasets.  \n",
      "- **Modality-Specific Bottlenecks**: Isolate data- versus compute-limited regimes for each modality and derive hybrid scaling laws that guide resource allocation in multi‐modal architectures.\n",
      "\n",
      "### 2. Novel Theoretical Models  \n",
      "- **Beyond Power Laws**: Investigate richer functional forms (e.g., double‐power, sigmoid‐capped) to model performance plateaus and phase transitions in very large models.  \n",
      "- **Data Quality Metrics**: Integrate measures of label noise, sample diversity, and distribution shift into scaling frameworks to predict robustness and generalization.  \n",
      "- **Algorithmic Complexity**: Theorize how optimizer dynamics, batch composition, and architectural motifs (e.g., sparse attention) influence scaling exponents and convergence rates.\n",
      "\n",
      "### 3. Efficient Scaling Strategies  \n",
      "- **Parameter-Efficient Training**: Extend scaling analysis to techniques such as LoRA, Adapter modules, and prompt tuning, quantifying trade-offs between parameter count and downstream task performance.  \n",
      "- **Compute-Aware Architectures**: Co-design models with hardware constraints in mind, using adaptive depth/width scaling and conditional computation to optimize FLOP‐to‐accuracy curves.  \n",
      "- **Automated Scaling Schedulers**: Develop meta-learning or reinforcement learning approaches that dynamically allocate compute, data, and hyperparameters to maintain GPU/TPU utilization efficiency across scales.\n",
      "\n",
      "### 4. Sustainability-Oriented Research  \n",
      "- **Energy-Accuracy Frontier**: Construct “green” scaling laws that map total carbon emissions and energy budgets to expected model performance, enabling eco-efficient procurement of compute.  \n",
      "- **Lifecycle Assessment**: Incorporate upstream (manufacturing, cooling) and downstream (deployment, inference) emissions into scaling models, guiding decisions on retraining versus fine-tuning.  \n",
      "- **Renewable Compute Integration**: Study the impact of intermittent energy sources on long-run training curves and develop checkpointing or elastic-compute methods tailored to sustainable infrastructure.\n",
      "\n",
      "By pursuing these avenues, the field can both deepen its theoretical understanding of scaling phenomena and foster practical, resource-aware advances in large-scale machine learning.\n",
      "Completed section: ## Methodologies for Analyzing Scaling\n",
      "\n",
      "### 1. Experimental Setups  \n",
      "- **Model Families**  \n",
      "  - Transformer variants (e.g., GPT-style, encoder–decoder) with systematically varied depth, width, and embedding size  \n",
      "  - Control for architectural factors (layer norm placement, activation functions)  \n",
      "- **Compute Environment**  \n",
      "  - Fixed GPU/TPU configurations (e.g., V100 × 16, A100 × 32)  \n",
      "  - Consistent mixed-precision training and batch-size schedules  \n",
      "- **Training Protocols**  \n",
      "  - Learning-rate warmup and decay schedules standardized across runs  \n",
      "  - Gradient clipping thresholds and optimizer hyperparameters (AdamW β₁/β₂ settings)  \n",
      "  - Early stopping on validation loss to prevent over-training  \n",
      "\n",
      "### 2. Metrics  \n",
      "- **Primary Loss Metrics**  \n",
      "  - Cross-entropy loss and token-level perplexity on held-out text  \n",
      "- **Downstream Task Performance**  \n",
      "  - Zero-shot/​few-shot accuracy (e.g., classification, multiple-choice QA)  \n",
      "  - BLEU/ROUGE for translation and summarization tasks  \n",
      "- **Calibration & Uncertainty**  \n",
      "  - Expected Calibration Error (ECE) over binned confidence scores  \n",
      "  - Negative log-likelihood (NLL) for probabilistic prediction quality  \n",
      "- **Compute Efficiency**  \n",
      "  - Training FLOPs vs. validation loss curves  \n",
      "  - Inference latency and throughput benchmarks  \n",
      "\n",
      "### 3. Data Regimes  \n",
      "- **Scale Tiers**  \n",
      "  - Sub-10 M tokens (“tiny”), 10 M–1 B tokens (“small”), 1 B–100 B tokens (“large”), >100 B tokens (“massive”)  \n",
      "- **Domain Variation**  \n",
      "  - Web-scraped corpora, curated encyclopedic text, code repositories  \n",
      "  - Balanced splits to measure domain transfer effects  \n",
      "- **Quality Controls**  \n",
      "  - Deduplication, toxicity filters, language identification pipelines  \n",
      "  - Noise injection experiments (e.g., shuffling, typo simulation)  \n",
      "\n",
      "### 4. Statistical Techniques  \n",
      "- **Scaling Law Fitting**  \n",
      "  - Power-law model:  \n",
      "    ```  \n",
      "    L(N, D, C) = A · N^α · D^β + ε  \n",
      "    ```  \n",
      "    • N = model parameters, D = training tokens, C = compute (FLOPs)  \n",
      "- **Regression & Model Selection**  \n",
      "  - Log-log linear regression to estimate exponents α, β  \n",
      "  - Information criteria (AIC, BIC) for comparing nested scaling models  \n",
      "- **Uncertainty Estimation**  \n",
      "  - Bootstrap resampling of runs (n ≥ 30) to derive confidence intervals for α, β  \n",
      "  - Bayesian hierarchical modeling to pool data across architectures  \n",
      "- **Hypothesis Testing**  \n",
      "  - ANOVA and pairwise t-tests on residuals to confirm significant trend differences  \n",
      "  - p-value correction (Bonferroni, Benjamini–Hochberg) when testing multiple metrics  \n",
      "\n",
      "This combined framework ensures rigorous measurement and validation of scaling trends, enabling reproducible insights into how model size, data quantity, and compute jointly influence language model performance.\n",
      "Completed section: ## Practical Implications for Model Development\n",
      "\n",
      "### 1. Model Size Selection  \n",
      "- **Performance vs. Cost Trade-off**  \n",
      "  - Scaling laws predict model performance \\(L\\propto N^{-\\alpha}\\) for parameter count \\(N\\).  \n",
      "  - Identify the “knee” in the scaling curve where marginal accuracy gains fall below cost increases.  \n",
      "  - Choose model sizes that maximize performance per FLOP rather than pursuing sheer scale.  \n",
      "- **Risk Management**  \n",
      "  - Smaller models can be iterated and tested faster, reducing development risk.  \n",
      "  - Reserve very large models for use cases with strict accuracy or capability requirements.\n",
      "\n",
      "### 2. Data Collection and Curation  \n",
      "- **Data Scaling Regime**  \n",
      "  - Under data-limited regimes, performance scales as \\(L\\propto D^{-\\beta}\\) for dataset size \\(D\\).  \n",
      "  - Allocate resources to gather high-quality, diverse data until returns diminish.  \n",
      "- **Quality vs. Quantity**  \n",
      "  - Curated, domain-specific data can outperform larger generic corpora once broad-domain performance saturates.  \n",
      "  - Use active learning or human-in-the-loop filtering to improve signal-to-noise ratio.  \n",
      "\n",
      "### 3. Compute Budgeting  \n",
      "- **Budget Allocation**  \n",
      "  - Distribute compute between model training, hyperparameter sweeps, and fine-tuning based on marginal gains observed via scaling exponents.  \n",
      "  - Plan for pretraining to consume ~80% of total FLOPs and fine-tuning at ~20%, adjusting as empirical results dictate.  \n",
      "- **Efficient Resource Utilization**  \n",
      "  - Leverage mixed-precision and model-parallel techniques to stretch budgets.  \n",
      "  - Monitor training curves for early stopping when loss plateaus according to predicted scaling behavior.\n",
      "\n",
      "### 4. Deployment Strategies  \n",
      "- **Latency vs. Accuracy**  \n",
      "  - Use smaller “student” models distilled from large “teacher” models in latency-sensitive settings.  \n",
      "  - Employ dynamic batching or adaptive computation time to balance throughput and responsiveness.  \n",
      "- **Modular and Hierarchical Architectures**  \n",
      "  - Split large models into modular components that can be loaded on demand to conserve hardware resources.  \n",
      "  - Implement cascade pipelines: fast, lightweight models handle common cases; escalating to large models only on “hard” inputs.  \n",
      "- **Continuous Improvement**  \n",
      "  - Track real-world performance to identify drift and feed new data into the next training cycle.  \n",
      "  - Update compute budgets and model sizes over time in response to evolving application requirements and cost curves.  \n",
      "\n",
      "By grounding design decisions in empirical scaling laws, teams can optimize resource allocation, accelerate development cycles, and deploy models that meet real-world constraints without overspending on marginal gains.\n",
      "Completed section: ## Limitations and Challenges\n",
      "\n",
      "### 1. Data and Representation Biases\n",
      "- **Uneven data distribution**  \n",
      "  Many scaling-law studies rely on web-scraped corpora and publicly available datasets that overrepresent certain languages, cultures, or domains, leading to skewed performance estimates and blind spots in underrepresented areas.  \n",
      "- **Algorithmic amplification**  \n",
      "  Models trained at scale can amplify existing societal biases (e.g., gender, ethnicity), and scaling laws rarely account for how bias metrics evolve with model size or data diversity.  \n",
      "- **Evaluation gaps**  \n",
      "  Standard benchmarks (e.g., GLUE, WMT) may not capture nuanced biases; without specialized bias benchmarks at different scales, it’s hard to measure whether larger models mitigate or exacerbate harmful behaviors.\n",
      "\n",
      "### 2. Environmental and Computational Costs\n",
      "- **Energy consumption**  \n",
      "  Training and inference at extreme scales drive up electricity usage and greenhouse-gas emissions. Few scaling-law analyses quantify the carbon footprint per performance increment or propose mitigation strategies (e.g., renewable energy mix, hardware efficiency).  \n",
      "- **Resource inequality**  \n",
      "  Access to tens of thousands of GPUs or TPUs is limited to large organizations; smaller labs or nonprofits cannot validate scaling trends, contributing to a concentration of research capabilities.  \n",
      "- **Hardware obsolescence**  \n",
      "  Rapid iteration on new model sizes can render specialized hardware (e.g., older GPU clusters) obsolete, raising electronic-waste concerns that current scaling-law research overlooks.\n",
      "\n",
      "### 3. Reproducibility and Transparency\n",
      "- **Compute budget opacity**  \n",
      "  Papers often omit detailed hardware configurations, energy accounting methods, or training run variability, making it difficult to reproduce results or compare scaling curves across studies.  \n",
      "- **Hyperparameter sensitivity**  \n",
      "  Performance at scale is highly sensitive to learning rates, batch sizes, and regularization schemes, yet many scaling-law formulations treat these as constants rather than variables, obscuring their impact.  \n",
      "- **Proprietary models and data**  \n",
      "  Closed-source efforts (e.g., internal datasets, custom accelerators) hinder third-party verification; scaling-law claims based on such assets lack open validation pathways.\n",
      "\n",
      "### 4. Diminishing Returns at Extreme Scales\n",
      "- **Performance plateaus**  \n",
      "  Empirical curves often show sublinear gains or plateaus beyond hundreds of billions of parameters, but the exact inflection points vary by task and data quality. Systematic exploration of where and why these plateaus occur remains incomplete.  \n",
      "- **Cost-benefit imbalance**  \n",
      "  Marginal improvements in metrics (e.g., perplexity, BLEU) can demand exponential increases in compute, making further scaling economically or environmentally untenable without significant algorithmic innovation.  \n",
      "- **Task specificity**  \n",
      "  Some tasks (e.g., commonsense reasoning, logical deduction) exhibit weaker scaling trends, suggesting that mere model enlargement cannot substitute for targeted architectural advances or inductive biases.\n",
      "\n",
      "By acknowledging and addressing these limitations—bias amplification, steep environmental costs, reproducibility hurdles, and the asymptotic returns of ever-larger models—future research can develop more robust, equitable, and sustainable scaling paradigms.\n",
      "Completed section: ## Background on Large Language Models\n",
      "\n",
      "### 1. Historical Evolution  \n",
      "- **Early Statistical Models (1990s–2010s)**  \n",
      "  - N-gram language models, count-based approaches  \n",
      "  - Limitations: sparse data, fixed context windows  \n",
      "- **Neural Sequence Models (2014–2017)**  \n",
      "  - RNNs and LSTMs for language modeling  \n",
      "  - Introduced continuous word embeddings (Word2Vec, GloVe)  \n",
      "  - Still struggled with long-range dependencies  \n",
      "- **Transformer Breakthrough (2017)**  \n",
      "  - “Attention Is All You Need” paper by Vaswani et al.  \n",
      "  - Self-attention mechanism enabled parallelization and long context  \n",
      "- **Rise of Generative Pretrained Models (2018–2023)**  \n",
      "  - GPT series (GPT-1 → GPT-4): scaling parameters from 117M to hundreds of billions  \n",
      "  - BERT, RoBERTa, T5: encoder/encoder-decoder variants for bidirectional and seq2seq tasks  \n",
      "  - Multimodal and retrieval-augmented extensions  \n",
      "\n",
      "### 2. Core Architecture  \n",
      "- **Transformer Encoder vs. Decoder**  \n",
      "  - Encoder-only (BERT): masked LM, bidirectional context  \n",
      "  - Decoder-only (GPT): autoregressive generation  \n",
      "  - Encoder-decoder (T5, BART): seq2seq tasks  \n",
      "- **Key Components**  \n",
      "  - Multi-head self-attention: captures diverse relationships in token sequences  \n",
      "  - Position-wise feedforward networks: nonlinearity and depth  \n",
      "  - Positional encodings: inject token position into the model  \n",
      "  - Layer normalization & residual connections: stabilize and speed up training  \n",
      "- **Scaling Dimensions**  \n",
      "  - Depth (number of layers)  \n",
      "  - Width (hidden size, number of attention heads)  \n",
      "  - Sequence length (context window)  \n",
      "  - Training compute (FLOPs) and data volume  \n",
      "\n",
      "### 3. Training Paradigms  \n",
      "- **Self-Supervised Pretraining**  \n",
      "  - Autoregressive objective: predict next token given prior context  \n",
      "  - Masked–language modeling: predict masked tokens from bidirectional context  \n",
      "  - Sequence-to-sequence learning: reconstruct corrupted inputs  \n",
      "- **Fine-Tuning & Adaptation**  \n",
      "  - Task-specific supervised fine-tuning with labeled data  \n",
      "  - Prompt-based learning and in-context examples (few-shot, zero-shot)  \n",
      "  - Parameter-efficient methods (adapters, LoRA, prefix tuning)  \n",
      "- **Reinforcement Learning from Human Feedback (RLHF)**  \n",
      "  - Human preferences guide model outputs via reward modeling  \n",
      "  - Iterative policy optimization (PPO) to align with desired behavior  \n",
      "- **Efficient Distributed Training**  \n",
      "  - Data parallelism, tensor (model) parallelism, pipeline parallelism  \n",
      "  - ZeRO, DeepSpeed, Megatron-LM optimizations  \n",
      "  - Quantization, pruning, memory‐efficient attention  \n",
      "\n",
      "### 4. Context for Scaling Discussions  \n",
      "Over the past decade, LLMs have evolved from narrow statistical predictors to massive, versatile generative engines. Their transformer-based architecture and self-supervised training unlocked unprecedented capabilities, but scaling further brings challenges in compute, data, and alignment. Understanding this evolution and the underlying design choices is essential for evaluating next-generation scaling strategies.\n",
      "Completed section: ## Theoretical Foundations of Scaling Laws\n",
      "\n",
      "### 1. Power‐Law Distributions  \n",
      "– **Empirical observation**  \n",
      "  • Model loss or error \\(L\\) often decreases as a power of model size \\(N\\):  \n",
      "    \\[\n",
      "      L(N) \\propto N^{-\\alpha}\\quad(\\alpha>0).\n",
      "    \\]  \n",
      "  • Similarly, dataset statistics (e.g.\\ word frequencies) follow Zipf’s law \\(p(k)\\propto k^{-s}\\), implying long‐tailed feature usage that benefits from larger capacity.  \n",
      "– **Mechanistic insight**  \n",
      "  • In a simple i.i.d. Gaussian model of features, the tail exponent \\(\\alpha\\) can be derived from the spectrum of covariance eigenvalues.  \n",
      "  • Random matrix theory shows that, as \\(N\\) grows, the learner “fills in” higher‐order eigenmodes at a rate set by the decay exponent of the data spectrum, yielding a universal scaling exponent.\n",
      "\n",
      "### 2. Neural Tangent Kernel (NTK)  \n",
      "– **Infinite‐width limit**  \n",
      "  • As hidden layer widths \\(\\to\\infty\\), gradient‐descent training dynamics on a deep network linearize around initialization. The network’s evolution is exactly described by a fixed kernel \\(K_{\\rm NTK}(x,x')\\).  \n",
      "– **Prediction of scaling exponents**  \n",
      "  • Under NTK theory, the generalization error after \\(T\\) steps on \\(N\\) parameters obeys  \n",
      "    \\[\n",
      "      L(N) \\approx \\sum_{i}\\exp\\bigl(-2\\lambda_i\\,T\\bigr)\\,(\\Delta f_i)^2,\n",
      "    \\]  \n",
      "    where \\(\\{\\lambda_i\\}\\) are kernel eigenvalues and \\(\\Delta f_i\\) are target‐function projections.  \n",
      "  • If \\(\\lambda_i\\sim i^{-\\beta}\\), then one recovers a power‐law decay \\(L(N)\\sim N^{-\\alpha}\\) with \\(\\alpha\\) determined by \\(\\beta\\).  \n",
      "– **Limitations**  \n",
      "  • NTK predicts only the “lazy‐training” regime; it fails to capture rich feature learning that emerges in finite‐width, large‐scale models.\n",
      "\n",
      "### 3. Double Descent Phenomenon  \n",
      "– **Classical U‐shaped risk**  \n",
      "  • In under‐parameterized models, increasing capacity reduces bias but increases variance, yielding a U‐shaped test error curve.  \n",
      "– **Interpolation threshold and second descent**  \n",
      "  • At the interpolation point—where model capacity equals number of training samples—test error peaks. Beyond this, further over‐parameterization paradoxically **improves** generalization.  \n",
      "– **Connection to scaling**  \n",
      "  • Large neural networks often operate deep in the second‐descent regime:  \n",
      "    – Increased width/depth reduces test error along a new descent branch.  \n",
      "    – Implicit regularization from SGD and smoothness of over‐parameterized solutions drive power‐law error decay.  \n",
      "– **Theoretical models**  \n",
      "  • Simplified random feature models and teacher–student setups show that double descent plus spectral bias combine to yield the empirically observed \\(L(N)\\propto N^{-\\alpha}\\) scaling.\n",
      "\n",
      "---\n",
      "\n",
      "**Synthesis:**  \n",
      "Power‐law exponents seen in scaling curves emerge from the interplay of (1) the spectrum of data covariance (power‐law tails), (2) linearized training dynamics (NTK eigenvalue decay), and (3) the over‐parameterization‐induced double‐descent effect. Together, these theories provide a cohesive account of why and how large neural models continue to improve along simple scaling laws.\n",
      "Completed section: ## Empirical Observations of Scaling Laws\n",
      "\n",
      "Major experimental studies have consistently demonstrated that model performance (e.g. validation loss or downstream-task accuracy) follows simple power-law relationships as a function of (1) the number of parameters \\(N\\), (2) the amount of training data \\(D\\), and (3) total compute \\(C\\). Below is a summary of key findings and benchmark curves.\n",
      "\n",
      "### 1. Parameter Scaling  \n",
      "- **Kaplan et al. (2020)**  \n",
      "  • Showed that test loss \\(L\\) for transformer language models obeys  \n",
      "    \\[ L(N) \\approx a_N \\, N^{-\\alpha_N} + L_\\infty, \\]  \n",
      "    with exponent \\(\\alpha_N \\approx 0.076\\).  \n",
      "  • On a log–log plot of loss vs. \\(N\\), curves for different dataset sizes collapse onto a single straight line.  \n",
      "- **Hestness et al. (2017)**  \n",
      "  • For convolutional image models, error rate falls as \\(N^{-\\alpha_{\\rm img}}\\) with \\(\\alpha_{\\rm img}\\approx 0.07\\text{–}0.1\\).  \n",
      "\n",
      "### 2. Data Scaling  \n",
      "- **Kaplan et al. (2020)**  \n",
      "  • Test loss scales as  \n",
      "    \\[ L(D) \\approx a_D \\, D^{-\\alpha_D} + L_\\infty, \\]  \n",
      "    with \\(\\alpha_D \\approx 0.095\\).  \n",
      "- **OpenAI GPT-3 (Brown et al., 2020)**  \n",
      "  • Trained on \\(\\sim3\\times10^{11}\\) tokens; observed continued power-law improvements in zero- and few-shot accuracy across tasks.  \n",
      "- **Chinchilla (Hoffmann et al., 2022)**  \n",
      "  • Rebalanced parameters vs. data: smaller models trained on more tokens outperform larger models with less data, following the same power-law trends.\n",
      "\n",
      "### 3. Compute-Optimal Frontier  \n",
      "- Derive \\(C \\approx N \\times D\\). Optimal allocation roughly satisfies  \n",
      "  \\[ D \\propto N^{0.74}, \\quad \\text{so that }C_{\\rm opt}\\propto N^{1.74}. \\]  \n",
      "- On a plot of loss vs. compute \\(C\\), the compute-optimal models trace another power law:  \n",
      "  \\[ L(C) \\approx a_C \\, C^{-\\alpha_C} + L_\\infty, \\]  \n",
      "  with \\(\\alpha_C\\approx0.05\\text{–}0.1\\).  \n",
      "- **Figure 1** (schematic): log–log curves of loss vs. \\(C\\), showing separate curves for sub-optimal and compute-optimal regimes.\n",
      "\n",
      "### 4. Benchmark Results  \n",
      "- **Language Modeling (perplexity or cross-entropy):**  \n",
      "  • GPT-3 175B: ~20B tokens, perplexity ~20 on WikiText-103.  \n",
      "  • Chinchilla 70B: ~1T tokens, perplexity ~18.  \n",
      "- **Downstream Tasks (few-shot accuracy):**  \n",
      "  • MMLU accuracy scales roughly as \\(N^{0.5}\\) in the 10B–100B parameter range.  \n",
      "  • Translation, summarization and question answering show similar power-law gains.  \n",
      "- **Vision Models:**  \n",
      "  • CLIP and ViT report linear improvements in top-1 accuracy on ImageNet as a function of \\(\\log N\\) or \\(\\log D\\), consistent with the same exponents found in language.\n",
      "\n",
      "Across domains, these empirical curves validate that simply increasing compute, data, or model size under the compute-optimal ratio reliably yields predictable, power-law improvements—with no sharp diminishing returns until extremely large scales.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Executive Summary\n",
       "\n",
       "This report investigates the scaling laws that govern the performance of large language models (LLMs) as a function of compute, model size, and data. Our objectives are to:\n",
       "\n",
       "- Characterize empirical relationships between model parameters, training tokens, and downstream task performance.  \n",
       "- Identify regimes of compute- and data-efficiency.  \n",
       "- Provide actionable guidance for researchers and industry practitioners on resource allocation and model design.\n",
       "\n",
       "Key Findings on LLM Scaling Laws  \n",
       "- **Power-Law Behavior**: Model performance (e.g., loss, perplexity) improves predictably with model size and training compute, following a power-law trend across multiple architectures.  \n",
       "- **Diminishing Returns**: Beyond certain compute thresholds, marginal gains per extra parameter or token decline, highlighting a regime shift from compute-bound to data-bound performance.  \n",
       "- **Optimal Compute Allocation**: There exists a “compute-optimal frontier” where the ratio of model size to training tokens maximizes performance per dollar spent.  \n",
       "- **Transfer and Fine-Tuning**: Larger pre-trained models yield steeper transfer curves, reducing fine-tuning data requirements for downstream tasks.\n",
       "\n",
       "Implications for Research and Industry  \n",
       "- **Resource Planning**: Organizations can forecast performance improvements and budget more accurately by adopting power-law projections.  \n",
       "- **Model Design**: The compute-optimal frontier helps determine when to scale model size versus invest in more training data or compute infrastructure.  \n",
       "- **Data Strategy**: In data-bound regimes, efforts should prioritize high-quality or diverse corpora rather than raw token count.  \n",
       "- **Competitive Advantage**: Early adopters of appropriately scaled models can achieve significant performance gains on specialized tasks with lower incremental costs.\n",
       "\n",
       "By grounding decisions in empirically validated scaling laws, stakeholders can optimize R&D investments, accelerate innovation, and balance performance objectives against resource constraints.\n",
       "\n",
       "---\n",
       "\n",
       "# Introduction\n",
       "\n",
       "Large language models (LLMs) are deep neural networks—typically built on transformer architectures—that are pretrained on vast corpora of text data to perform a wide range of natural language processing tasks. By learning statistical patterns and contextual relationships across billions or even trillions of tokens, LLMs can generate coherent text, answer questions, translate languages, summarize documents, and adapt to many downstream applications with minimal fine-tuning.\n",
       "\n",
       "Understanding how these models behave as they grow in size (“scaling behavior”) is critical for both researchers and practitioners. Empirical scaling laws reveal predictable improvements in performance with increasing model parameters, training data, and compute resources. At the same time, scaling can introduce emergent capabilities, shifting cost-benefit trade-offs, and new risks such as amplification of biases or vulnerabilities. A clear grasp of scaling behavior informs decisions about resource allocation, model deployment, and alignment strategies, ensuring that model development remains both efficient and responsible.\n",
       "\n",
       "This report investigates the scaling behavior of LLMs with three main goals:\n",
       "1. Characterize how performance metrics (e.g., perplexity, downstream task accuracy) systematically improve with model size, data volume, and compute budgets.  \n",
       "2. Identify key points of diminishing returns and explore strategies—such as model sparsity or data curation—to mitigate escalating costs.  \n",
       "3. Assess broader implications of scaling, including emergent behaviors, ethical considerations, and sustainability, to inform best practices for future model development.\n",
       "\n",
       "By mapping out the relationship between scale and capability, this report aims to guide stakeholders in making informed, cost-effective, and responsible choices in the era of ever-larger language models.\n",
       "\n",
       "---\n",
       "\n",
       "## Background on Large Language Models\n",
       "\n",
       "### 1. Historical Evolution  \n",
       "- **Early Statistical Models (1990s–2010s)**  \n",
       "  - N-gram language models, count-based approaches  \n",
       "  - Limitations: sparse data, fixed context windows  \n",
       "- **Neural Sequence Models (2014–2017)**  \n",
       "  - RNNs and LSTMs for language modeling  \n",
       "  - Introduced continuous word embeddings (Word2Vec, GloVe)  \n",
       "  - Still struggled with long-range dependencies  \n",
       "- **Transformer Breakthrough (2017)**  \n",
       "  - “Attention Is All You Need” paper by Vaswani et al.  \n",
       "  - Self-attention mechanism enabled parallelization and long context  \n",
       "- **Rise of Generative Pretrained Models (2018–2023)**  \n",
       "  - GPT series (GPT-1 → GPT-4): scaling parameters from 117M to hundreds of billions  \n",
       "  - BERT, RoBERTa, T5: encoder/encoder-decoder variants for bidirectional and seq2seq tasks  \n",
       "  - Multimodal and retrieval-augmented extensions  \n",
       "\n",
       "### 2. Core Architecture  \n",
       "- **Transformer Encoder vs. Decoder**  \n",
       "  - Encoder-only (BERT): masked LM, bidirectional context  \n",
       "  - Decoder-only (GPT): autoregressive generation  \n",
       "  - Encoder-decoder (T5, BART): seq2seq tasks  \n",
       "- **Key Components**  \n",
       "  - Multi-head self-attention: captures diverse relationships in token sequences  \n",
       "  - Position-wise feedforward networks: nonlinearity and depth  \n",
       "  - Positional encodings: inject token position into the model  \n",
       "  - Layer normalization & residual connections: stabilize and speed up training  \n",
       "- **Scaling Dimensions**  \n",
       "  - Depth (number of layers)  \n",
       "  - Width (hidden size, number of attention heads)  \n",
       "  - Sequence length (context window)  \n",
       "  - Training compute (FLOPs) and data volume  \n",
       "\n",
       "### 3. Training Paradigms  \n",
       "- **Self-Supervised Pretraining**  \n",
       "  - Autoregressive objective: predict next token given prior context  \n",
       "  - Masked–language modeling: predict masked tokens from bidirectional context  \n",
       "  - Sequence-to-sequence learning: reconstruct corrupted inputs  \n",
       "- **Fine-Tuning & Adaptation**  \n",
       "  - Task-specific supervised fine-tuning with labeled data  \n",
       "  - Prompt-based learning and in-context examples (few-shot, zero-shot)  \n",
       "  - Parameter-efficient methods (adapters, LoRA, prefix tuning)  \n",
       "- **Reinforcement Learning from Human Feedback (RLHF)**  \n",
       "  - Human preferences guide model outputs via reward modeling  \n",
       "  - Iterative policy optimization (PPO) to align with desired behavior  \n",
       "- **Efficient Distributed Training**  \n",
       "  - Data parallelism, tensor (model) parallelism, pipeline parallelism  \n",
       "  - ZeRO, DeepSpeed, Megatron-LM optimizations  \n",
       "  - Quantization, pruning, memory‐efficient attention  \n",
       "\n",
       "### 4. Context for Scaling Discussions  \n",
       "Over the past decade, LLMs have evolved from narrow statistical predictors to massive, versatile generative engines. Their transformer-based architecture and self-supervised training unlocked unprecedented capabilities, but scaling further brings challenges in compute, data, and alignment. Understanding this evolution and the underlying design choices is essential for evaluating next-generation scaling strategies.\n",
       "\n",
       "---\n",
       "\n",
       "## Empirical Observations of Scaling Laws\n",
       "\n",
       "Major experimental studies have consistently demonstrated that model performance (e.g. validation loss or downstream-task accuracy) follows simple power-law relationships as a function of (1) the number of parameters \\(N\\), (2) the amount of training data \\(D\\), and (3) total compute \\(C\\). Below is a summary of key findings and benchmark curves.\n",
       "\n",
       "### 1. Parameter Scaling  \n",
       "- **Kaplan et al. (2020)**  \n",
       "  • Showed that test loss \\(L\\) for transformer language models obeys  \n",
       "    \\[ L(N) \\approx a_N \\, N^{-\\alpha_N} + L_\\infty, \\]  \n",
       "    with exponent \\(\\alpha_N \\approx 0.076\\).  \n",
       "  • On a log–log plot of loss vs. \\(N\\), curves for different dataset sizes collapse onto a single straight line.  \n",
       "- **Hestness et al. (2017)**  \n",
       "  • For convolutional image models, error rate falls as \\(N^{-\\alpha_{\\rm img}}\\) with \\(\\alpha_{\\rm img}\\approx 0.07\\text{–}0.1\\).  \n",
       "\n",
       "### 2. Data Scaling  \n",
       "- **Kaplan et al. (2020)**  \n",
       "  • Test loss scales as  \n",
       "    \\[ L(D) \\approx a_D \\, D^{-\\alpha_D} + L_\\infty, \\]  \n",
       "    with \\(\\alpha_D \\approx 0.095\\).  \n",
       "- **OpenAI GPT-3 (Brown et al., 2020)**  \n",
       "  • Trained on \\(\\sim3\\times10^{11}\\) tokens; observed continued power-law improvements in zero- and few-shot accuracy across tasks.  \n",
       "- **Chinchilla (Hoffmann et al., 2022)**  \n",
       "  • Rebalanced parameters vs. data: smaller models trained on more tokens outperform larger models with less data, following the same power-law trends.\n",
       "\n",
       "### 3. Compute-Optimal Frontier  \n",
       "- Derive \\(C \\approx N \\times D\\). Optimal allocation roughly satisfies  \n",
       "  \\[ D \\propto N^{0.74}, \\quad \\text{so that }C_{\\rm opt}\\propto N^{1.74}. \\]  \n",
       "- On a plot of loss vs. compute \\(C\\), the compute-optimal models trace another power law:  \n",
       "  \\[ L(C) \\approx a_C \\, C^{-\\alpha_C} + L_\\infty, \\]  \n",
       "  with \\(\\alpha_C\\approx0.05\\text{–}0.1\\).  \n",
       "- **Figure 1** (schematic): log–log curves of loss vs. \\(C\\), showing separate curves for sub-optimal and compute-optimal regimes.\n",
       "\n",
       "### 4. Benchmark Results  \n",
       "- **Language Modeling (perplexity or cross-entropy):**  \n",
       "  • GPT-3 175B: ~20B tokens, perplexity ~20 on WikiText-103.  \n",
       "  • Chinchilla 70B: ~1T tokens, perplexity ~18.  \n",
       "- **Downstream Tasks (few-shot accuracy):**  \n",
       "  • MMLU accuracy scales roughly as \\(N^{0.5}\\) in the 10B–100B parameter range.  \n",
       "  • Translation, summarization and question answering show similar power-law gains.  \n",
       "- **Vision Models:**  \n",
       "  • CLIP and ViT report linear improvements in top-1 accuracy on ImageNet as a function of \\(\\log N\\) or \\(\\log D\\), consistent with the same exponents found in language.\n",
       "\n",
       "Across domains, these empirical curves validate that simply increasing compute, data, or model size under the compute-optimal ratio reliably yields predictable, power-law improvements—with no sharp diminishing returns until extremely large scales.\n",
       "\n",
       "---\n",
       "\n",
       "## Theoretical Foundations of Scaling Laws\n",
       "\n",
       "### 1. Power‐Law Distributions  \n",
       "– **Empirical observation**  \n",
       "  • Model loss or error \\(L\\) often decreases as a power of model size \\(N\\):  \n",
       "    \\[\n",
       "      L(N) \\propto N^{-\\alpha}\\quad(\\alpha>0).\n",
       "    \\]  \n",
       "  • Similarly, dataset statistics (e.g.\\ word frequencies) follow Zipf’s law \\(p(k)\\propto k^{-s}\\), implying long‐tailed feature usage that benefits from larger capacity.  \n",
       "– **Mechanistic insight**  \n",
       "  • In a simple i.i.d. Gaussian model of features, the tail exponent \\(\\alpha\\) can be derived from the spectrum of covariance eigenvalues.  \n",
       "  • Random matrix theory shows that, as \\(N\\) grows, the learner “fills in” higher‐order eigenmodes at a rate set by the decay exponent of the data spectrum, yielding a universal scaling exponent.\n",
       "\n",
       "### 2. Neural Tangent Kernel (NTK)  \n",
       "– **Infinite‐width limit**  \n",
       "  • As hidden layer widths \\(\\to\\infty\\), gradient‐descent training dynamics on a deep network linearize around initialization. The network’s evolution is exactly described by a fixed kernel \\(K_{\\rm NTK}(x,x')\\).  \n",
       "– **Prediction of scaling exponents**  \n",
       "  • Under NTK theory, the generalization error after \\(T\\) steps on \\(N\\) parameters obeys  \n",
       "    \\[\n",
       "      L(N) \\approx \\sum_{i}\\exp\\bigl(-2\\lambda_i\\,T\\bigr)\\,(\\Delta f_i)^2,\n",
       "    \\]  \n",
       "    where \\(\\{\\lambda_i\\}\\) are kernel eigenvalues and \\(\\Delta f_i\\) are target‐function projections.  \n",
       "  • If \\(\\lambda_i\\sim i^{-\\beta}\\), then one recovers a power‐law decay \\(L(N)\\sim N^{-\\alpha}\\) with \\(\\alpha\\) determined by \\(\\beta\\).  \n",
       "– **Limitations**  \n",
       "  • NTK predicts only the “lazy‐training” regime; it fails to capture rich feature learning that emerges in finite‐width, large‐scale models.\n",
       "\n",
       "### 3. Double Descent Phenomenon  \n",
       "– **Classical U‐shaped risk**  \n",
       "  • In under‐parameterized models, increasing capacity reduces bias but increases variance, yielding a U‐shaped test error curve.  \n",
       "– **Interpolation threshold and second descent**  \n",
       "  • At the interpolation point—where model capacity equals number of training samples—test error peaks. Beyond this, further over‐parameterization paradoxically **improves** generalization.  \n",
       "– **Connection to scaling**  \n",
       "  • Large neural networks often operate deep in the second‐descent regime:  \n",
       "    – Increased width/depth reduces test error along a new descent branch.  \n",
       "    – Implicit regularization from SGD and smoothness of over‐parameterized solutions drive power‐law error decay.  \n",
       "– **Theoretical models**  \n",
       "  • Simplified random feature models and teacher–student setups show that double descent plus spectral bias combine to yield the empirically observed \\(L(N)\\propto N^{-\\alpha}\\) scaling.\n",
       "\n",
       "---\n",
       "\n",
       "**Synthesis:**  \n",
       "Power‐law exponents seen in scaling curves emerge from the interplay of (1) the spectrum of data covariance (power‐law tails), (2) linearized training dynamics (NTK eigenvalue decay), and (3) the over‐parameterization‐induced double‐descent effect. Together, these theories provide a cohesive account of why and how large neural models continue to improve along simple scaling laws.\n",
       "\n",
       "---\n",
       "\n",
       "## Methodologies for Analyzing Scaling\n",
       "\n",
       "### 1. Experimental Setups  \n",
       "- **Model Families**  \n",
       "  - Transformer variants (e.g., GPT-style, encoder–decoder) with systematically varied depth, width, and embedding size  \n",
       "  - Control for architectural factors (layer norm placement, activation functions)  \n",
       "- **Compute Environment**  \n",
       "  - Fixed GPU/TPU configurations (e.g., V100 × 16, A100 × 32)  \n",
       "  - Consistent mixed-precision training and batch-size schedules  \n",
       "- **Training Protocols**  \n",
       "  - Learning-rate warmup and decay schedules standardized across runs  \n",
       "  - Gradient clipping thresholds and optimizer hyperparameters (AdamW β₁/β₂ settings)  \n",
       "  - Early stopping on validation loss to prevent over-training  \n",
       "\n",
       "### 2. Metrics  \n",
       "- **Primary Loss Metrics**  \n",
       "  - Cross-entropy loss and token-level perplexity on held-out text  \n",
       "- **Downstream Task Performance**  \n",
       "  - Zero-shot/​few-shot accuracy (e.g., classification, multiple-choice QA)  \n",
       "  - BLEU/ROUGE for translation and summarization tasks  \n",
       "- **Calibration & Uncertainty**  \n",
       "  - Expected Calibration Error (ECE) over binned confidence scores  \n",
       "  - Negative log-likelihood (NLL) for probabilistic prediction quality  \n",
       "- **Compute Efficiency**  \n",
       "  - Training FLOPs vs. validation loss curves  \n",
       "  - Inference latency and throughput benchmarks  \n",
       "\n",
       "### 3. Data Regimes  \n",
       "- **Scale Tiers**  \n",
       "  - Sub-10 M tokens (“tiny”), 10 M–1 B tokens (“small”), 1 B–100 B tokens (“large”), >100 B tokens (“massive”)  \n",
       "- **Domain Variation**  \n",
       "  - Web-scraped corpora, curated encyclopedic text, code repositories  \n",
       "  - Balanced splits to measure domain transfer effects  \n",
       "- **Quality Controls**  \n",
       "  - Deduplication, toxicity filters, language identification pipelines  \n",
       "  - Noise injection experiments (e.g., shuffling, typo simulation)  \n",
       "\n",
       "### 4. Statistical Techniques  \n",
       "- **Scaling Law Fitting**  \n",
       "  - Power-law model:  \n",
       "    ```  \n",
       "    L(N, D, C) = A · N^α · D^β + ε  \n",
       "    ```  \n",
       "    • N = model parameters, D = training tokens, C = compute (FLOPs)  \n",
       "- **Regression & Model Selection**  \n",
       "  - Log-log linear regression to estimate exponents α, β  \n",
       "  - Information criteria (AIC, BIC) for comparing nested scaling models  \n",
       "- **Uncertainty Estimation**  \n",
       "  - Bootstrap resampling of runs (n ≥ 30) to derive confidence intervals for α, β  \n",
       "  - Bayesian hierarchical modeling to pool data across architectures  \n",
       "- **Hypothesis Testing**  \n",
       "  - ANOVA and pairwise t-tests on residuals to confirm significant trend differences  \n",
       "  - p-value correction (Bonferroni, Benjamini–Hochberg) when testing multiple metrics  \n",
       "\n",
       "This combined framework ensures rigorous measurement and validation of scaling trends, enabling reproducible insights into how model size, data quantity, and compute jointly influence language model performance.\n",
       "\n",
       "---\n",
       "\n",
       "## Practical Implications for Model Development\n",
       "\n",
       "### 1. Model Size Selection  \n",
       "- **Performance vs. Cost Trade-off**  \n",
       "  - Scaling laws predict model performance \\(L\\propto N^{-\\alpha}\\) for parameter count \\(N\\).  \n",
       "  - Identify the “knee” in the scaling curve where marginal accuracy gains fall below cost increases.  \n",
       "  - Choose model sizes that maximize performance per FLOP rather than pursuing sheer scale.  \n",
       "- **Risk Management**  \n",
       "  - Smaller models can be iterated and tested faster, reducing development risk.  \n",
       "  - Reserve very large models for use cases with strict accuracy or capability requirements.\n",
       "\n",
       "### 2. Data Collection and Curation  \n",
       "- **Data Scaling Regime**  \n",
       "  - Under data-limited regimes, performance scales as \\(L\\propto D^{-\\beta}\\) for dataset size \\(D\\).  \n",
       "  - Allocate resources to gather high-quality, diverse data until returns diminish.  \n",
       "- **Quality vs. Quantity**  \n",
       "  - Curated, domain-specific data can outperform larger generic corpora once broad-domain performance saturates.  \n",
       "  - Use active learning or human-in-the-loop filtering to improve signal-to-noise ratio.  \n",
       "\n",
       "### 3. Compute Budgeting  \n",
       "- **Budget Allocation**  \n",
       "  - Distribute compute between model training, hyperparameter sweeps, and fine-tuning based on marginal gains observed via scaling exponents.  \n",
       "  - Plan for pretraining to consume ~80% of total FLOPs and fine-tuning at ~20%, adjusting as empirical results dictate.  \n",
       "- **Efficient Resource Utilization**  \n",
       "  - Leverage mixed-precision and model-parallel techniques to stretch budgets.  \n",
       "  - Monitor training curves for early stopping when loss plateaus according to predicted scaling behavior.\n",
       "\n",
       "### 4. Deployment Strategies  \n",
       "- **Latency vs. Accuracy**  \n",
       "  - Use smaller “student” models distilled from large “teacher” models in latency-sensitive settings.  \n",
       "  - Employ dynamic batching or adaptive computation time to balance throughput and responsiveness.  \n",
       "- **Modular and Hierarchical Architectures**  \n",
       "  - Split large models into modular components that can be loaded on demand to conserve hardware resources.  \n",
       "  - Implement cascade pipelines: fast, lightweight models handle common cases; escalating to large models only on “hard” inputs.  \n",
       "- **Continuous Improvement**  \n",
       "  - Track real-world performance to identify drift and feed new data into the next training cycle.  \n",
       "  - Update compute budgets and model sizes over time in response to evolving application requirements and cost curves.  \n",
       "\n",
       "By grounding design decisions in empirical scaling laws, teams can optimize resource allocation, accelerate development cycles, and deploy models that meet real-world constraints without overspending on marginal gains.\n",
       "\n",
       "---\n",
       "\n",
       "## Limitations and Challenges\n",
       "\n",
       "### 1. Data and Representation Biases\n",
       "- **Uneven data distribution**  \n",
       "  Many scaling-law studies rely on web-scraped corpora and publicly available datasets that overrepresent certain languages, cultures, or domains, leading to skewed performance estimates and blind spots in underrepresented areas.  \n",
       "- **Algorithmic amplification**  \n",
       "  Models trained at scale can amplify existing societal biases (e.g., gender, ethnicity), and scaling laws rarely account for how bias metrics evolve with model size or data diversity.  \n",
       "- **Evaluation gaps**  \n",
       "  Standard benchmarks (e.g., GLUE, WMT) may not capture nuanced biases; without specialized bias benchmarks at different scales, it’s hard to measure whether larger models mitigate or exacerbate harmful behaviors.\n",
       "\n",
       "### 2. Environmental and Computational Costs\n",
       "- **Energy consumption**  \n",
       "  Training and inference at extreme scales drive up electricity usage and greenhouse-gas emissions. Few scaling-law analyses quantify the carbon footprint per performance increment or propose mitigation strategies (e.g., renewable energy mix, hardware efficiency).  \n",
       "- **Resource inequality**  \n",
       "  Access to tens of thousands of GPUs or TPUs is limited to large organizations; smaller labs or nonprofits cannot validate scaling trends, contributing to a concentration of research capabilities.  \n",
       "- **Hardware obsolescence**  \n",
       "  Rapid iteration on new model sizes can render specialized hardware (e.g., older GPU clusters) obsolete, raising electronic-waste concerns that current scaling-law research overlooks.\n",
       "\n",
       "### 3. Reproducibility and Transparency\n",
       "- **Compute budget opacity**  \n",
       "  Papers often omit detailed hardware configurations, energy accounting methods, or training run variability, making it difficult to reproduce results or compare scaling curves across studies.  \n",
       "- **Hyperparameter sensitivity**  \n",
       "  Performance at scale is highly sensitive to learning rates, batch sizes, and regularization schemes, yet many scaling-law formulations treat these as constants rather than variables, obscuring their impact.  \n",
       "- **Proprietary models and data**  \n",
       "  Closed-source efforts (e.g., internal datasets, custom accelerators) hinder third-party verification; scaling-law claims based on such assets lack open validation pathways.\n",
       "\n",
       "### 4. Diminishing Returns at Extreme Scales\n",
       "- **Performance plateaus**  \n",
       "  Empirical curves often show sublinear gains or plateaus beyond hundreds of billions of parameters, but the exact inflection points vary by task and data quality. Systematic exploration of where and why these plateaus occur remains incomplete.  \n",
       "- **Cost-benefit imbalance**  \n",
       "  Marginal improvements in metrics (e.g., perplexity, BLEU) can demand exponential increases in compute, making further scaling economically or environmentally untenable without significant algorithmic innovation.  \n",
       "- **Task specificity**  \n",
       "  Some tasks (e.g., commonsense reasoning, logical deduction) exhibit weaker scaling trends, suggesting that mere model enlargement cannot substitute for targeted architectural advances or inductive biases.\n",
       "\n",
       "By acknowledging and addressing these limitations—bias amplification, steep environmental costs, reproducibility hurdles, and the asymptotic returns of ever-larger models—future research can develop more robust, equitable, and sustainable scaling paradigms.\n",
       "\n",
       "---\n",
       "\n",
       "## Future Directions\n",
       "\n",
       "### 1. Multimodal Scaling Laws  \n",
       "- **Cross-Modal Interactions**: Empirically characterize how scaling behaviors transfer between modalities (e.g., vision ↔ language) and identify unified power-law exponents.  \n",
       "- **Composite Training Curves**: Develop joint scaling functions that predict performance when training on combined text, image, audio, and video datasets.  \n",
       "- **Modality-Specific Bottlenecks**: Isolate data- versus compute-limited regimes for each modality and derive hybrid scaling laws that guide resource allocation in multi‐modal architectures.\n",
       "\n",
       "### 2. Novel Theoretical Models  \n",
       "- **Beyond Power Laws**: Investigate richer functional forms (e.g., double‐power, sigmoid‐capped) to model performance plateaus and phase transitions in very large models.  \n",
       "- **Data Quality Metrics**: Integrate measures of label noise, sample diversity, and distribution shift into scaling frameworks to predict robustness and generalization.  \n",
       "- **Algorithmic Complexity**: Theorize how optimizer dynamics, batch composition, and architectural motifs (e.g., sparse attention) influence scaling exponents and convergence rates.\n",
       "\n",
       "### 3. Efficient Scaling Strategies  \n",
       "- **Parameter-Efficient Training**: Extend scaling analysis to techniques such as LoRA, Adapter modules, and prompt tuning, quantifying trade-offs between parameter count and downstream task performance.  \n",
       "- **Compute-Aware Architectures**: Co-design models with hardware constraints in mind, using adaptive depth/width scaling and conditional computation to optimize FLOP‐to‐accuracy curves.  \n",
       "- **Automated Scaling Schedulers**: Develop meta-learning or reinforcement learning approaches that dynamically allocate compute, data, and hyperparameters to maintain GPU/TPU utilization efficiency across scales.\n",
       "\n",
       "### 4. Sustainability-Oriented Research  \n",
       "- **Energy-Accuracy Frontier**: Construct “green” scaling laws that map total carbon emissions and energy budgets to expected model performance, enabling eco-efficient procurement of compute.  \n",
       "- **Lifecycle Assessment**: Incorporate upstream (manufacturing, cooling) and downstream (deployment, inference) emissions into scaling models, guiding decisions on retraining versus fine-tuning.  \n",
       "- **Renewable Compute Integration**: Study the impact of intermittent energy sources on long-run training curves and develop checkpointing or elastic-compute methods tailored to sustainable infrastructure.\n",
       "\n",
       "By pursuing these avenues, the field can both deepen its theoretical understanding of scaling phenomena and foster practical, resource-aware advances in large-scale machine learning.\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "Over the past decade, empirical scaling laws have revealed remarkably consistent power-law relationships between model performance and three primary resources—parameter count, training data volume, and compute. Key insights include:  \n",
       "- Predictable gains: Each doubling of compute or data tends to yield a reproducible improvement in cross-task performance metrics.  \n",
       "- Diminishing returns boundaries: While raw scale drives breakthroughs, marginal gains shrink at extreme sizes, highlighting the importance of algorithmic and architectural innovations.  \n",
       "- Emergent capabilities: At critical scales, models exhibit qualitatively new abilities—reasoning, code synthesis, multi-modal understanding—that were not present at smaller sizes.\n",
       "\n",
       "These findings carry profound implications for the future trajectory of AI:  \n",
       "- Roadmap for innovation: Scaling laws afford a quantitative blueprint for resource allocation, enabling research teams to plan compute and data investments more strategically.  \n",
       "- Acceleration of transformative applications: Continued scale-driven improvements promise advances in healthcare, climate modeling, scientific discovery, and creative industries.  \n",
       "- New frontiers in safety and alignment: As models become more capable, ensuring robust grounding, interpretability, and alignment with human values grows ever more critical.\n",
       "\n",
       "Stakeholders across academia, industry, and government must consider:  \n",
       "- Researchers and developers: Balance brute-force scaling with efficiency gains via sparse architectures, data curation, and novel training paradigms.  \n",
       "- Corporations and startups: Invest in modular AI stacks and cloud-based infrastructure to remain agile, while forging partnerships to access specialized datasets and hardware.  \n",
       "- Policymakers and regulators: Craft adaptive frameworks that encourage innovation yet mitigate risks—covering transparency standards, accountability measures, and mechanisms for international coordination.  \n",
       "- Society and educators: Prepare the workforce through reskilling initiatives, emphasize AI literacy, and foster public dialogues on ethical deployment.\n",
       "\n",
       "By heeding these insights and aligning strategic decisions with scaling-law principles, stakeholders can steer AI development toward more powerful, beneficial, and responsible outcomes."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke\n",
    "state = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(state[\"final_report\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c4cde",
   "metadata": {},
   "source": [
    "### Functional API code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2340c579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)\n",
    "\n",
    "\n",
    "@task\n",
    "def orchestrator(topic: str):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return report_sections.sections\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call(section: Section):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    result = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write a report section.\"),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {section.name} and description: {section.description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return result.content\n",
    "\n",
    "\n",
    "@task\n",
    "def synthesizer(completed_sections: list[str]):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "    final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "    return final_report\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "def orchestrator_worker(topic: str):\n",
    "    sections = orchestrator(topic).result()\n",
    "    section_futures = [llm_call(section) for section in sections]\n",
    "    final_report = synthesizer(\n",
    "        [section_fut.result() for section_fut in section_futures]\n",
    "    ).result()\n",
    "    return final_report\n",
    "\n",
    "\n",
    "# Invoke\n",
    "report = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\n",
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fd3d5",
   "metadata": {},
   "source": [
    "## Evaluator-optimizer\n",
    "\n",
    "In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a [human-in-the-loop](/oss/python/langgraph/add-human-in-the-loop) determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated.\n",
    "\n",
    "Evaluator-optimizer workflows are commonly used when there's particular success criteria for a task, but iteration is required to meet that criteria. For example, there's not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.\n",
    "\n",
    "![png](01_langgraph_workflows_and_agents_files/evaluator_optimizer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "07fb791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    joke: str\n",
    "    topic: str\n",
    "    feedback: str\n",
    "    funny_or_not: str\n",
    "\n",
    "\n",
    "# Schema for structured output to use in evaluation\n",
    "class Feedback(BaseModel):\n",
    "    grade: Literal[\"funny\", \"not funny\"] = Field(\n",
    "        description=\"Decide if the joke is funny or not.\",\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "evaluator = llm.with_structured_output(Feedback)\n",
    "\n",
    "\n",
    "# Define graph nodes\n",
    "# First LLM call: Generate joke\n",
    "def llm_call_generator(state: State):\n",
    "    \"\"\"LLM generates a joke\"\"\"\n",
    "\n",
    "    if state.get(\"feedback\"):\n",
    "        msg = llm.invoke(\n",
    "            f\"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}\"\n",
    "        )\n",
    "    else:\n",
    "        msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n",
    "    return {\"joke\": msg.content}\n",
    "\n",
    "# Second LLM call: Evaluate joke\n",
    "def llm_call_evaluator(state: State):\n",
    "    \"\"\"LLM evaluates the joke\"\"\"\n",
    "\n",
    "    grade = evaluator.invoke(f\"Grade the joke {state['joke']}\")\n",
    "    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\n",
    "\n",
    "\n",
    "# Conditional edge function to route back to joke generator or end based upon feedback \n",
    "# from the evaluator\n",
    "def route_joke(state: State):\n",
    "    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\n",
    "\n",
    "    if state[\"funny_or_not\"] == \"funny\":\n",
    "        return \"Accepted\"\n",
    "    elif state[\"funny_or_not\"] == \"not funny\":\n",
    "        return \"Rejected + Feedback\"\n",
    "\n",
    "\n",
    "# Build workflow\n",
    "optimizer_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "optimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\n",
    "optimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "optimizer_builder.add_edge(START, \"llm_call_generator\")\n",
    "optimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\n",
    "optimizer_builder.add_conditional_edges(\n",
    "    \"llm_call_evaluator\",\n",
    "    route_joke,\n",
    "    {  # Name returned by route_joke : Name of next node to visit\n",
    "        \"Accepted\": END,\n",
    "        \"Rejected + Feedback\": \"llm_call_generator\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile the workflow\n",
    "optimizer_workflow = optimizer_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the workflow\n",
    "display(Image(optimizer_workflow.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4b4b88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why was the cat sitting on the computer?  \n",
      "Because it wanted to keep an eye on the mouse!\n"
     ]
    }
   ],
   "source": [
    "# Invoke\n",
    "state = optimizer_workflow.invoke({\"topic\": \"Cats\"})\n",
    "print(state[\"joke\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efada48",
   "metadata": {},
   "source": [
    "## Agents\n",
    "\n",
    "Agents are typically implemented as an LLM performing actions using [tools](/oss/python/langchain/tools). They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.\n",
    "\n",
    "![png](01_langgraph_workflows_and_agents_files/agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e7dceb",
   "metadata": {},
   "source": [
    "### Langgraph agents implementation example with tools and memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7612d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# Define tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Divide a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2aa3bc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import add_messages\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    BaseMessage,\n",
    "    ToolCall,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69466663",
   "metadata": {},
   "outputs": [],
   "source": [
    "@task\n",
    "def call_llm(messages: list[BaseMessage]):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    return llm_with_tools.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n",
    "            )\n",
    "        ]\n",
    "        + messages\n",
    "    )\n",
    "\n",
    "\n",
    "@task\n",
    "def call_tool(tool_call: ToolCall):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "    tool = tools_by_name[tool_call[\"name\"]]\n",
    "    return tool.invoke(tool_call)\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "def agent(messages: list[BaseMessage]):\n",
    "    llm_response = call_llm(messages).result()\n",
    "\n",
    "    while True:\n",
    "        if not llm_response.tool_calls:\n",
    "            break\n",
    "\n",
    "        # Execute tools\n",
    "        tool_result_futures = [\n",
    "            call_tool(tool_call) for tool_call in llm_response.tool_calls\n",
    "        ]\n",
    "        tool_results = [fut.result() for fut in tool_result_futures]\n",
    "        messages = add_messages(messages, [llm_response, *tool_results])\n",
    "        llm_response = call_llm(messages).result()\n",
    "\n",
    "    messages = add_messages(messages, llm_response)\n",
    "    return messages\n",
    "\n",
    "\n",
    "# Invoke\n",
    "messages = [HumanMessage(content=\"Add 3 and 4.\")]\n",
    "for chunk in agent.stream(messages, stream_mode=\"updates\"):\n",
    "    print(chunk)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc383e05",
   "metadata": {},
   "source": [
    "## Attribution & License\n",
    "\n",
    "This tutorial adapts content from the official LangGraph documentation and tutorials.\n",
    "\n",
    "**Source**: LangGraph Official Documentation  \n",
    "**Copyright**: © 2024 LangChain, Inc.  \n",
    "**License**: MIT License  \n",
    "**Original Materials**: [LangGraph Documentation](https://docs.langchain.com/oss/python/langgraph/overview)\n",
    "\n",
    "The original materials are licensed under the MIT License:\n",
    "- Full license text: [MIT License](https://github.com/langchain-ai/langgraph/blob/main/LICENSE)\n",
    "- Permission granted to use, copy, modify, and distribute with attribution\n",
    "\n",
    "**Aparsoft's Adaptations**: This tutorial has been adapted and enhanced by Aparsoft Private Limited with additional examples, explanations, and use cases specific to our developer community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc119c",
   "metadata": {},
   "source": [
    "## 📞 Get Help & Connect\n",
    "\n",
    "### 🎓 Learning & Community\n",
    "- **YouTube:** [@aparsoft-ai](https://youtube.com/@aparsoft-ai) - Main tutorial channel\n",
    "- **Github:** [Join our community](https://github.com/aparsoft) - Check our repos and examples (aparsoft-tutorial-resources)\n",
    "- **GitHub Discussions:** Ask questions about the code\n",
    "- **LinkedIn:** [/company/aparsoft](https://linkedin.com/company/aparsoft) - Articles and tips\n",
    "- **X (formerly Twitter):** [@aparsoft](https://x.com/AparsoftPvtLtd) - Tutorials and updates"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
